{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZRM5mYk4pVKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkOqy5yMoscS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/best_model.pth'\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "# Function to filter out examples with text longer than 1000 characters\n",
        "def filter_long_text(example):\n",
        "    return len(example['text']) <= 1000\n",
        "\n",
        "# Apply the filter to the test split only\n",
        "filtered_test_dataset = dataset['test'].filter(filter_long_text)\n",
        "\n",
        "# Split the test dataset into 90% and 10%\n",
        "test_size = len(filtered_test_dataset)\n",
        "\n",
        "# Calculate the number of examples to keep as the new test set (10%)\n",
        "num_to_keep = int(0.1 * test_size)\n",
        "\n",
        "# Shuffle and select 10% of the test set for evaluation\n",
        "indices = list(range(test_size))\n",
        "random.shuffle(indices)\n",
        "test_indices = indices[:num_to_keep]\n",
        "\n",
        "# Select the corresponding examples from the filtered test set\n",
        "new_test_set = filtered_test_dataset.select(test_indices)\n",
        "\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    return param_size / 1e6  # Convert to MB\n",
        "\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    labels = [label for label in examples[\"label\"]]\n",
        "    label_texts = [\"positive\" if label == 1 else \"negative\" for label in labels]\n",
        "    model_labels = tokenizer(label_texts, max_length=10, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = model_labels\n",
        "    return model_inputs\n",
        "\n",
        "# Load the FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Tokenize the new test dataset\n",
        "tokenized_test_dataset = new_test_set.map(preprocess_function, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
        "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
        "    labels = torch.stack([torch.tensor(item[\"labels\"]) for item in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "eval_dataloader = DataLoader(tokenized_test_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "# Function to evaluate the model and compute validation loss and accuracy\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=10)\n",
        "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                if pred.strip().lower() == label.strip().lower():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "    return avg_eval_loss, accuracy\n",
        "\n",
        "# Evaluate the original model on GPU\n",
        "gpu_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(gpu_device)\n",
        "\n",
        "original_model_size = get_model_size(model)\n",
        "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
        "\n",
        "original_eval_loss, original_accuracy = evaluate_model(model, eval_dataloader, gpu_device)\n",
        "print(f\"Original Model Evaluation Loss: {original_eval_loss:.4f}\")\n",
        "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
        "\n",
        "# Perform dynamic quantization and move the model to CPU\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model.cpu(), {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Evaluate the size of the quantized model\n",
        "quantized_model_size = get_model_size(quantized_model)\n",
        "print(f\"Quantized model size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "# Evaluate the quantized model on the CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "quantized_eval_loss, quantized_accuracy = evaluate_model(quantized_model, eval_dataloader, cpu_device)\n",
        "print(f\"Quantized Model Evaluation Loss: {quantized_eval_loss:.4f}\")\n",
        "print(f\"Quantized Model Accuracy: {quantized_accuracy:.4f}\")\n",
        "\n",
        "# Compare model sizes, evaluation losses, and accuracy\n",
        "size_reduction = 100 * (original_model_size - quantized_model_size) / original_model_size\n",
        "loss_increase = 100 * (quantized_eval_loss - original_eval_loss) / original_eval_loss\n",
        "accuracy_decrease = 100 * (original_accuracy - quantized_accuracy) / original_accuracy\n",
        "\n",
        "print(f\"Size reduction: {size_reduction:.2f}%\")\n",
        "print(f\"Loss increase: {loss_increase:.2f}%\")\n",
        "print(f\"Accuracy decrease: {accuracy_decrease:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Original model size: 990.31 MB\n",
        "Original Model Evaluation Loss: 0.0158\n",
        "Original Model Accuracy: 0.9466\n",
        "Quantized model size: 98.89 MB"
      ],
      "metadata": {
        "id": "y20zokz1p63P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/best_model.pth'\n",
        "checkpoint = torch.load(model_path)"
      ],
      "metadata": {
        "id": "KnpQIf7gYU93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(checkpoint)"
      ],
      "metadata": {
        "id": "Y2F5C1-xYVlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints"
      ],
      "metadata": {
        "id": "EY4FsK_nYj9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}