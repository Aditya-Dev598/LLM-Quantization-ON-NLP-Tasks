{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sAns4Mqb8O"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install py7zr\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SBYKCFPbqiuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset, load_metric\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# Load the best model\n",
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/best_model.pth'\n",
        "checkpoint = torch.load(model_path)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Function to evaluate the model and compute validation loss and ROUGE score\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
        "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Compute ROUGE scores\n",
        "            rouge.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(dataloader)\n",
        "    rouge_score = rouge.compute()\n",
        "    return avg_eval_loss, rouge_score\n",
        "\n",
        "# Evaluate the size of the model before quantization\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    param_bytes = 0\n",
        "    for param in model.parameters():\n",
        "        param_bytes += param.nelement() * param.element_size()\n",
        "        param_size += param_bytes\n",
        "    return param_size / 1e6  # Convert to MB\n",
        "\n",
        "# Prepare the evaluation dataset and dataloader\n",
        "dataset = load_dataset(\"samsum\")\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "# Preprocessing for the SAMsum dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "\n",
        "# Move the original model to GPU and evaluate\n",
        "gpu_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(gpu_device)\n",
        "\n",
        "original_model_size = get_model_size(model)\n",
        "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
        "\n",
        "original_eval_loss, original_rouge_score = evaluate_model(model, eval_dataloader, gpu_device)\n",
        "print(f\"Original Model Evaluation Loss: {original_eval_loss:.4f}\")\n",
        "print(f\"Original Model ROUGE Score: {original_rouge_score}\")\n",
        "\n",
        "# Perform dynamic quantization and move the model to CPU\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model.cpu(), {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Evaluate the size of the quantized model\n",
        "quantized_model_size = get_model_size(quantized_model)\n",
        "print(f\"Quantized model size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "# Evaluate the quantized model on the CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "quantized_eval_loss, quantized_rouge_score = evaluate_model(quantized_model, eval_dataloader, cpu_device)\n",
        "print(f\"Quantized Model Evaluation Loss: {quantized_eval_loss:.4f}\")\n",
        "print(f\"Quantized Model ROUGE Score: {quantized_rouge_score}\")\n",
        "\n",
        "# Compare model sizes and evaluation losses\n",
        "size_reduction = 100 * (original_model_size - quantized_model_size) / original_model_size\n",
        "print(f\"Size reduction: {size_reduction:.2f}%\")\n",
        "loss_increase = 100 * (quantized_eval_loss - original_eval_loss) / original_eval_loss\n",
        "print(f\"Loss increase: {loss_increase:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Hopun8u-qngP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset, load_metric\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# Load the best model\n",
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/best_model.pth'\n",
        "checkpoint = torch.load(model_path)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Function to evaluate the model and compute validation loss and ROUGE score\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
        "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            # Compute ROUGE scores\n",
        "            rouge.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(dataloader)\n",
        "    rouge_score = rouge.compute()\n",
        "    return avg_eval_loss, rouge_score\n",
        "\n",
        "# Evaluate the size of the model before quantization\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    param_bytes = 0\n",
        "    for param in model.parameters():\n",
        "        param_bytes += param.nelement() * param.element_size()\n",
        "        param_size += param_bytes\n",
        "    return param_size / 1e6  # Convert to MB\n",
        "\n",
        "# Prepare the evaluation dataset and dataloader\n",
        "dataset = load_dataset(\"samsum\")\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "# Preprocessing for the SAMsum dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=64, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "-kDuhIO1FKdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "device = \"cpu\"\n",
        "def evaluate_random_input(model, tokenizer, eval_dataset, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Choose a random sample from the dataset\n",
        "    random_idx = random.randint(0, len(eval_dataset) - 1)\n",
        "    sample = eval_dataset[random_idx]\n",
        "    input_dialogue = sample[\"dialogue\"]\n",
        "    ground_truth_summary = sample[\"summary\"]\n",
        "\n",
        "    # Preprocess the input dialogue\n",
        "    input_text = f\"summarize: {input_dialogue}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate the output summary\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=128)\n",
        "\n",
        "    # Decode the input, output, and ground truth\n",
        "    decoded_input = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print input, model output, and ground truth\n",
        "    print(f\"Input Dialogue: {decoded_input}\")\n",
        "    print(f\"Model Output: {decoded_output}\")\n",
        "    print(f\"Ground Truth Summary: {ground_truth_summary}\")\n",
        "\n",
        "# Evaluate and print a random input from the SAMsum validation set\n",
        "evaluate_random_input(model, tokenizer, eval_dataset, device)"
      ],
      "metadata": {
        "id": "x4ddCHhCFU8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmnj3MIeGbw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}