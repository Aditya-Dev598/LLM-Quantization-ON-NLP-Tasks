{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "WM_LKKWAiBNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X_nD9IyOh8nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, epoch, train_losses, eval_losses, filepath):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),  # Ensure model is a valid PyTorch model object\n",
        "        'optimizer_state_dict': optimizer.state_dict(),  # Ensure optimizer is a valid optimizer object\n",
        "        'scheduler_state_dict': scheduler.state_dict(),  # Ensure scheduler is a valid scheduler object\n",
        "        'train_losses': train_losses,  # Ensure train_losses is a list or value\n",
        "        'eval_losses': eval_losses,  # Ensure eval_losses is a list or value\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved at {filepath}\")"
      ],
      "metadata": {
        "id": "Wj-O9v-z0Azd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, optimizer, scheduler, filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    eval_losses = checkpoint['eval_losses']\n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "    return model, optimizer, scheduler, epoch, train_losses, eval_losses"
      ],
      "metadata": {
        "id": "d4sxfpnsIObt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Load the GSM8K dataset\n",
        "dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "\n",
        "# Load the FLAN-T5 Small model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Preprocessing function for the dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [question for question in examples[\"question\"]]\n",
        "    targets = [answer for answer in examples[\"answer\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "# Custom collate function to handle padding and conversion to tensors\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# Set up DataLoader\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=collate_fn)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "\n",
        "# Training setup\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * 10  # Assuming 20 epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "# Early stopping setup\n",
        "best_eval_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 3  # Early stopping patience\n",
        "\n",
        "# Lists to store training and evaluation losses\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#model, optimizer, scheduler, epochss, train_losses, eval_losses = load_model(\n",
        "#)\n",
        "model.to(device)\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / num_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch: {epoch+1}, Average Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_losses.append(avg_eval_loss)\n",
        "    print(f\"Epoch: {epoch+1}, Evaluation Loss: {avg_eval_loss}\")\n",
        "\n",
        "    # Check for early stopping and save the best model\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        patience_counter = 0  # Reset patience counter\n",
        "        save_model(\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            train_losses=train_losses,\n",
        "            eval_losses=eval_losses,\n",
        "            filepath = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x GSM8K/best_model.pth'\n",
        "        )\n",
        "        print(f\"Best model saved with eval loss {avg_eval_loss:.4f} at epoch {epoch+1}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break"
      ],
      "metadata": {
        "id": "9FpPVfcmfRGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the GSM8K dataset\n",
        "dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "\n",
        "# Load the FLAN-T5 Small model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing function for the dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"solve: {question}\" for question in examples[\"question\"]]\n",
        "    targets = [answer for answer in examples[\"answer\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the test dataset\n",
        "tokenized_test_dataset = dataset['test'].map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "# Set up DataLoader for the test dataset\n",
        "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1)  # Batch size 1 for single evaluation\n",
        "\n",
        "# Load the saved checkpoint (if needed)\n",
        "checkpoint_path = '/content/drive/MyDrive/LLM Models/FLAN-T5-base x GSM8K/best_model.pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to evaluate a single random example from the test dataset\n",
        "def evaluate_random_example(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "\n",
        "    # Select a random example from the dataset\n",
        "    random_idx = random.randint(0, len(dataset) - 1)\n",
        "    example = dataset[random_idx]\n",
        "\n",
        "    # Get the input and ground truth\n",
        "    input_text = example['input_ids']\n",
        "    ground_truth = example['labels']\n",
        "\n",
        "    # Decode the input and ground truth\n",
        "    input_decoded = tokenizer.decode(input_text, skip_special_tokens=True)\n",
        "    ground_truth_decoded = tokenizer.decode(ground_truth, skip_special_tokens=True)\n",
        "\n",
        "    # Generate the model's prediction\n",
        "    input_tensor = torch.tensor([input_text]).to(device)\n",
        "    attention_mask = torch.tensor([example['attention_mask']]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_tensor, attention_mask=attention_mask, max_length=128)\n",
        "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Question: {input_decoded}\")\n",
        "    print(f\"Ground Truth Answer: {ground_truth_decoded}\")\n",
        "    print(f\"Model's Predicted Answer: {prediction}\")\n",
        "\n",
        "# Evaluate a random example\n",
        "evaluate_random_example(model, tokenizer, tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "XB2KVHUqIlOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets"
      ],
      "metadata": {
        "id": "4CMbVpT2sNfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
        "\n",
        "input_text = \"\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "hEsMkh6LEJjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def load_losses(filepath):\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    # Extract the train and eval losses\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    eval_losses = checkpoint['eval_losses']\n",
        "\n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "\n",
        "    return train_losses, eval_losses\n",
        "\n",
        "# Usage\n",
        "train_losses, eval_losses = load_losses('/content/drive/MyDrive/LLM Models/FLAN-T5-base x IMDB/best_model.pth')"
      ],
      "metadata": {
        "id": "sEoG9ErNFi6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses"
      ],
      "metadata": {
        "id": "9SgSUAcX4OvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_losses"
      ],
      "metadata": {
        "id": "bf3KZdfj4a_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JGB_gLd4g9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}