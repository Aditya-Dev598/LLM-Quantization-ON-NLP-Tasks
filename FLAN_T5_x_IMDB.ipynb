{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRxdugHqXIU-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9hSk2q77scyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from datasets import load_dataset, concatenate_datasets, ClassLabel\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# Function to filter out examples with text longer than 2000 characters\n",
        "def filter_long_text(example):\n",
        "    return len(example['text']) <= 1000\n",
        "\n",
        "# Apply the filter to the train and test splits\n",
        "filtered_train_dataset = dataset['train'].filter(filter_long_text)\n",
        "filtered_test_dataset = dataset['test'].filter(filter_long_text)\n",
        "\n",
        "# Split the test dataset into 90% and 10%\n",
        "test_size = len(filtered_test_dataset)\n",
        "\n",
        "# Calculate the number of examples to move to the training set\n",
        "num_to_add = int(0.9 * test_size)\n",
        "\n",
        "# Shuffle and select 90% of the test set\n",
        "indices = list(range(test_size))\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:num_to_add]\n",
        "test_indices = indices[num_to_add:]\n",
        "\n",
        "# Select the corresponding examples from the filtered test set\n",
        "additional_train_set = filtered_test_dataset.select(train_indices)\n",
        "new_test_set = filtered_test_dataset.select(test_indices)\n",
        "\n",
        "# Concatenate 90% of the filtered test set with the filtered train set\n",
        "new_train_set = concatenate_datasets([filtered_train_dataset, additional_train_set])\n",
        "\n",
        "# Check the results\n",
        "print(\"Original train set size:\", len(dataset['train']))\n",
        "print(\"Original test set size:\", len(dataset['test']))\n",
        "\n",
        "print(\"Filtered train set size:\", len(filtered_train_dataset))\n",
        "print(\"Filtered test set size:\", len(filtered_test_dataset))\n",
        "\n",
        "print(\"New train set size after adding 90% of the test set:\", len(new_train_set))\n",
        "print(\"New test set size after removing 90% of the test set:\", len(new_test_set))"
      ],
      "metadata": {
        "id": "bLrljKdQaerW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Convert the classification task into a text-to-text task\n",
        "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Map the sentiment labels to target text (e.g., \"positive\" or \"negative\")\n",
        "    labels = [label for label in examples[\"label\"]]\n",
        "    label_texts = [\"positive\" if label == 1 else \"negative\" for label in labels]\n",
        "    model_labels = tokenizer(label_texts, max_length=10, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = model_labels\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\", \"label\"])"
      ],
      "metadata": {
        "id": "1j8cg6ExXZTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
        "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
        "    labels = torch.stack([torch.tensor(item[\"labels\"]) for item in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ],
      "metadata": {
        "id": "VzmVB9Y8d_t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].shuffle()  # Using a subset for demonstration\n",
        "eval_dataset = tokenized_datasets[\"test\"].shuffle()\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "QU4UWdWAX323"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "total_steps = len(train_dataloader) * 10  # Assuming 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "nH1HYdVvbpYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, epoch, train_losses, eval_losses, filepath):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),  # Ensure model is a valid PyTorch model object\n",
        "        'optimizer_state_dict': optimizer.state_dict(),  # Ensure optimizer is a valid optimizer object\n",
        "        'scheduler_state_dict': scheduler.state_dict(),  # Ensure scheduler is a valid scheduler object\n",
        "        'train_losses': train_losses,  # Ensure train_losses is a list or value\n",
        "        'eval_losses': eval_losses,  # Ensure eval_losses is a list or value\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved at {filepath}\")"
      ],
      "metadata": {
        "id": "SYrbddLgcPOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, optimizer, scheduler, filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    eval_losses = checkpoint['eval_losses']\n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "    return model, optimizer, scheduler, epoch, train_losses, eval_losses"
      ],
      "metadata": {
        "id": "UDs2gs3TjiUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, scheduler, epochs, train_losses, eval_losses = load_model(model, optimizer, scheduler, r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x IMDB/best_model.pth')"
      ],
      "metadata": {
        "id": "LoqVbt_Rjn0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(epochs)"
      ],
      "metadata": {
        "id": "Yu65kFdSrG8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs,10):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_losses.append(avg_eval_loss)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        save_model(\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            train_losses=train_losses,\n",
        "            eval_losses=eval_losses,\n",
        "            filepath = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x IMDB/best_model.pth'\n",
        "        )\n",
        "        print(f\"Best model saved with eval loss {avg_eval_loss:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_eval_loss}\")"
      ],
      "metadata": {
        "id": "0y7YAlemjyAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "best_eval_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_losses.append(avg_eval_loss)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        save_model(\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            train_losses=train_losses,\n",
        "            eval_losses=eval_losses,\n",
        "            filepath = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x IMDB/best_model.pth'\n",
        "        )\n",
        "        print(f\"Best model saved with eval loss {avg_eval_loss:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_eval_loss}\")"
      ],
      "metadata": {
        "id": "c_TSEhkhbtL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4dSmWoehaWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}