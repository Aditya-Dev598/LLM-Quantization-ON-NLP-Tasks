{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Xjf7BJFADdPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TDDRX_K3CeT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the best model\n",
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x GSM8K/best_model.pth'\n",
        "checkpoint = torch.load(model_path)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate the size of the model before quantization\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    param_bytes = 0\n",
        "    for param in model.parameters():\n",
        "        param_bytes += param.nelement() * param.element_size()\n",
        "        param_size += param_bytes\n",
        "    return param_size / 1e6  # Convert to MB\n",
        "\n",
        "original_model_size = get_model_size(model)\n",
        "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [question for question in examples[\"question\"]]\n",
        "    targets = [answer for answer in examples[\"answer\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "eval_dataset = tokenized_datasets[\"test\"]\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "\n",
        "# Function to evaluate the model and compute validation loss\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(dataloader)\n",
        "    return avg_eval_loss\n",
        "\n",
        "# Move the original model to device and evaluate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "original_eval_loss = evaluate_model(model, eval_dataloader, device)\n",
        "print(f\"Original Model Evaluation Loss: {original_eval_loss:.4f}\")\n",
        "\n",
        "# Perform dynamic quantization and move the model to CPU\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model.cpu(), {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Evaluate the size of the quantized model\n",
        "quantized_model_size = get_model_size(quantized_model)\n",
        "print(f\"Quantized model size: {quantized_model_size:.2f} MB\")\n",
        "\n",
        "# Ensure the quantized model is evaluated on the CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "quantized_eval_loss = evaluate_model(quantized_model, eval_dataloader, cpu_device)\n",
        "print(f\"Quantized Model Evaluation Loss: {quantized_eval_loss:.4f}\")\n",
        "\n",
        "# Compare model sizes and evaluation losses\n",
        "size_reduction = 100 * (original_model_size - quantized_model_size) / original_model_size\n",
        "print(f\"Size reduction: {size_reduction:.2f}%\")\n",
        "loss_increase = 100 * (quantized_eval_loss - original_eval_loss) / original_eval_loss\n",
        "print(f\"Loss increase: {loss_increase:.2f}%\")"
      ],
      "metadata": {
        "id": "ty7v7Tt9LK4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the best model\n",
        "model_path = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x GSM8K/best_model.pth'\n",
        "checkpoint = torch.load(model_path)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Move the original model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example from GSM8K dataset\n",
        "example = \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
        "\n",
        "# Function to generate output from the model\n",
        "def generate_answer(model, tokenizer, example, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer.encode(example, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "        outputs = model.generate(inputs, max_length=128)\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Get output from the original model\n",
        "original_output = generate_answer(model, tokenizer, example, device)\n",
        "print(f\"Original Model Output: {original_output}\")\n",
        "\n",
        "# Perform dynamic quantization and move the model to CPU\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model.cpu(), {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Ensure the quantized model is evaluated on the CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Get output from the quantized model\n",
        "quantized_output = generate_answer(quantized_model, tokenizer, example, cpu_device)\n",
        "print(f\"Quantized Model Output: {quantized_output}\")"
      ],
      "metadata": {
        "id": "XIw46f-RO2Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUHqIl5InfG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}