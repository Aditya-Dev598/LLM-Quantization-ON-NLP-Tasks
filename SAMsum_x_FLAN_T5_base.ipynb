{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install py7zr"
      ],
      "metadata": {
        "id": "UCM4IDFrebc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, epoch, train_losses, eval_losses, filepath):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),  # Ensure model is a valid PyTorch model object\n",
        "        'optimizer_state_dict': optimizer.state_dict(),  # Ensure optimizer is a valid optimizer object\n",
        "        'scheduler_state_dict': scheduler.state_dict(),  # Ensure scheduler is a valid scheduler object\n",
        "        'train_losses': train_losses,  # Ensure train_losses is a list or value\n",
        "        'eval_losses': eval_losses,  # Ensure eval_losses is a list or value\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved at {filepath}\")"
      ],
      "metadata": {
        "id": "4458NzkJUtHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")"
      ],
      "metadata": {
        "id": "Arw3oVn7gG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LfR8hteKUUOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the FLAN-T5 base model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "_VR5Tz9lQ73e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize the summaries (labels)\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
      ],
      "metadata": {
        "id": "44lcm_UQRAZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/tokenized_samsum\"\n",
        "tokenized_datasets.save_to_disk(save_directory)"
      ],
      "metadata": {
        "id": "LY01rMAET_5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "tokenized_datasets = load_from_disk(\"/content/drive/MyDrive/LLM Models/FLAN-T5-small x CNN-DailyMail/tokenized_cnn_dailymail\")"
      ],
      "metadata": {
        "id": "3rDt_HaTJl-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
        "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
        "    labels = torch.stack([torch.tensor(item[\"labels\"]) for item in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ],
      "metadata": {
        "id": "WyhSvnz0b28t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "05cBt-fzRNzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Set up learning rate scheduler\n",
        "total_steps = len(train_dataloader) * 10  # Assuming 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "qmWoPHUoSAT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "best_eval_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_losses.append(avg_eval_loss)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_eval_loss < best_eval_loss:\n",
        "        best_eval_loss = avg_eval_loss\n",
        "        save_model(\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            train_losses=train_losses,\n",
        "            eval_losses=eval_losses,\n",
        "            filepath = r'/content/drive/MyDrive/LLM Models/FLAN-T5-base x SAMsum/best_model.pth'\n",
        "        )\n",
        "        print(f\"Best model saved with eval loss {avg_eval_loss:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_eval_loss}\")"
      ],
      "metadata": {
        "id": "kulTpUmeSCws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "\n",
        "# Select a single example from the test set\n",
        "single_data = dataset[\"test\"][10]\n",
        "\n",
        "# Display the dialogue and summary\n",
        "print(\"Dialogue:\")\n",
        "print(single_data[\"dialogue\"])\n",
        "print(\"\\nGround Truth Summary:\")\n",
        "print(single_data[\"summary\"])"
      ],
      "metadata": {
        "id": "AAlX2hi1MwFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input for the model\n",
        "input_text = f\"summarize: {single_data['dialogue']}\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "# Move inputs to GPU if available\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}"
      ],
      "metadata": {
        "id": "mfjvJlALgzSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode the generated summary\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nGenerated Summary:\")\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "id": "PXxmZPPxhBWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "# Compute ROUGE scores\n",
        "scores = rouge.compute(predictions=[generated_summary], references=[single_data[\"summary\"]])\n",
        "\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "1CXEG5CBhJxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "sxKl1Ub7hn1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare lists to store predictions and references\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Loop through the evaluation dataset and generate summaries\n",
        "for example in dataset[\"validation\"]:\n",
        "    input_text = f\"summarize: {example['dialogue']}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Move inputs to GPU if available\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the generated summary\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append the generated summary and reference to the respective lists\n",
        "    predictions.append(generated_summary)\n",
        "    references.append(example[\"summary\"])\n",
        "\n",
        "# Compute ROUGE scores for the entire dataset\n",
        "scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(\"ROUGE Scores for the entire evaluation dataset:\")\n",
        "print(scores['rouge1'])\n",
        "print(scores['rouge2'])\n",
        "print(scores['rougeL'])\n",
        "print(scores['rougeLsum'])"
      ],
      "metadata": {
        "id": "5qnuwZGfh72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1zSVzpsl3hH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}